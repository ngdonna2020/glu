{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudocode : GLU and its variants in place of traditional activation functions\n",
    "\n",
    "This is a formal pseudocode description based on the paper **GLU Variants Improve Transformer** by Noam Shazeer (2020). We can see how the GLU-based feed-forward sublayers introduced in Noam Shazeerâ€™s paper differ from the traditional feed-forward layers in transformer architectures by using Gated Linear Units (GLU) or its variants in place of traditional activation functions like ReLU or GELU.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Transformer Feed-Forward Network (FFN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that ReLU(x) = max(0,x) and we can approximate GELU(x)= 0.5x(1+tanh($\\sqrt{(2/ \\pi}(x+0.044715(x^3) )))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional FFN Layer (using ReLU or GELU)\n",
    "def FFN(x, W1, W2):\n",
    "    # Step 1: Linear Transformation\n",
    "    h = x @ W1\n",
    "    \n",
    "    # Step 2: Activation (ReLU or GELU)\n",
    "    h_activated = activation_function(h)  # ReLU or GELU\n",
    "\n",
    "    # Step 3: Output Linear Transformation\n",
    "    out = h_activated @ W2\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice you can import torch as below to apply activation functions directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Traditional FFN Layer using ReLU or GELU\n",
    "def FFN(x, W1, W2, activation='relu'):\n",
    "    # Step 1: Linear Transformation\n",
    "    h = x @ W1\n",
    "    \n",
    "    # Step 2: Apply the activation function (ReLU or GELU)\n",
    "    if activation == 'relu':\n",
    "        h_activated = F.relu(h)  # ReLU: max(0, x)\n",
    "    elif activation == 'gelu':\n",
    "        # GELU activation as defined by the approximation formula\n",
    "        h_activated = 0.5 * h * (1 + torch.tanh((torch.sqrt(torch.tensor(2 / torch.pi)) * (h + 0.044715 * h**3))))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function specified. Use 'relu' or 'gelu'.\")\n",
    "\n",
    "    # Step 3: Output Linear Transformation\n",
    "    out = h_activated @ W2\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLU-Based FFN Layer (from the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that Gated Linear Units consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLU-Based FFN Layer\n",
    "def GLU(x, W, V):\n",
    "    # Step 1: Compute Linear Transformations\n",
    "    a = x @ W\n",
    "    b = x @ V\n",
    "\n",
    "    # Step 2: Apply Sigmoid Activation for Gating\n",
    "    gate = sigmoid(a)\n",
    "\n",
    "    # Step 3: Element-wise product (gated linear unit)\n",
    "    h_glu = gate * b\n",
    "\n",
    "    return h_glu\n",
    "\n",
    "# Pseudocode for the FFNGLU Layer using GLU within the transformer feed-forward block\n",
    "def FFNGLU(x, W, V, W2):\n",
    "    # Step 1: GLU Transformation (element-wise product)\n",
    "    h_glu = GLU(x, W, V)\n",
    "\n",
    "    # Step 2: Apply final linear transformation\n",
    "    out = h_glu @ W2\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLU Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReGLU Variant (using ReLU instead of Sigmoid)\n",
    "def ReGLU(x, W, V):\n",
    "    a = x @ W\n",
    "    b = x @ V\n",
    "    gate = relu(a)  # ReLU activation as the gating function\n",
    "    h_glu = gate * b\n",
    "    return h_glu\n",
    "\n",
    "# GEGLU Variant (using GELU instead of Sigmoid)\n",
    "def GEGLU(x, W, V):\n",
    "    a = x @ W\n",
    "    b = x @ V\n",
    "    gate = gelu(a)  # GELU activation as the gating function\n",
    "    h_glu = gate * b\n",
    "    return h_glu\n",
    "\n",
    "# SwiGLU Variant (using Swish activation)\n",
    "def SwiGLU(x, W, V, beta):\n",
    "    a = x @ W\n",
    "    b = x @ V\n",
    "    gate = swish(a, beta)  # Swish activation as the gating function\n",
    "    h_glu = gate * b\n",
    "    return h_glu\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
